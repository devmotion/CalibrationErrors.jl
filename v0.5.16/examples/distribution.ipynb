{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Distribution of calibration error estimates\n",
    "\n",
    "You are seeing the\n",
    "notebook output generated by\n",
    "[Literate.jl](https://github.com/fredrikekre/Literate.jl) from the\n",
    "[Julia source file](https://github.com/devmotion/CalibrationErrors.jl/blob/master/examples/distribution/script.jl).\n",
    "The corresponding\n",
    "HTML output can be viewed [here](https://devmotion.github.io/CalibrationErrors.jl/dev/examples/distribution/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Packages"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using CairoMakie\n",
    "using CalibrationErrors\n",
    "using Distances\n",
    "using Distributions\n",
    "using StatsBase\n",
    "\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "using Statistics\n",
    "\n",
    "using CairoMakie.AbstractPlotting.ColorSchemes: Dark2_8"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "This example is taken from the publication\n",
    "[\"Calibration tests in multi-class classification: A unifying framework\"](https://proceedings.neurips.cc/paper/2019/hash/1c336b8080f82bcc2cd2499b4c57261d-Abstract.html)\n",
    "by Widmann, Lindsten, and Zachariah (2019).\n",
    "\n",
    "We estimate calibration errors of the model\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   g(X) &\\sim \\mathrm{Dir}(\\alpha),\\\\\n",
    "   Z &\\sim \\mathrm{Ber}(\\pi),\\\\\n",
    "   Y \\,|\\, g(X) = \\gamma, Z = 1 &\\sim \\mathrm{Categorical}(\\beta),\\\\\n",
    "   Y \\,|\\, g(X) = \\gamma, Z = 0 &\\sim \\mathrm{Categorical}(\\gamma),\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\alpha \\in \\mathbb{R}_{>0}^m$ determines the distribution of\n",
    "predictions $g(X)$, $\\pi > 0$ determines the degree of miscalibration, and\n",
    "$\\beta$ defines a fixed categorical distribution.\n",
    "\n",
    "Here we consider only the choices $\\alpha = (0.1, \\ldots, 0.1)$, mimicking a\n",
    "distribution after training that is pushed towards the edges of the\n",
    "probability simplex, and $\\beta = (1, 0, \\ldots, 0)$.\n",
    "\n",
    "In our experiments we sample 250 predictions from the Dirichlet distribution\n",
    "$\\textrm{Dir}(\\alpha)$, and then we generate corresponding labels according to\n",
    "the model stated above, for different choices of $\\pi$ and number of classes $m$.\n",
    "\n",
    "We evaluate the standard estimators of expected calibration error (ECE) based on a\n",
    "uniform binning scheme and a data-dependent binning scheme, and the biased estimator of\n",
    "the squared kernel calibration error (SKCE), the quadratic unbiased estimator of\n",
    "the SKCE, and the linear unbiased estimator of the SKCE for a specific choice of\n",
    "matrix-valued kernels.\n",
    "\n",
    "The sampling procedure and the evaluation are repeated 100 times, to obtain a sample\n",
    "of 100 estimates for each considered setting of $\\pi$ and $m$.\n",
    "\n",
    "For our choice of $\\alpha$ and $\\beta$, the analytical ECE with respect to the\n",
    "total variation distance $\\|.\\|_{\\mathrm{TV}}$ is\n",
    "$$\n",
    "\\mathrm{ECE}_{\\mathrm{TV}} = \\frac{\\pi(m-1)}{m}.\n",
    "$$\n",
    "\n",
    "## Estimates"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function estimates(estimator, π::Real, m::Int)\n",
    "    # cache array for predictions, modified predictions, and labels\n",
    "    predictions = [Vector{Float64}(undef, m) for _ in 1:250]\n",
    "    targets = Vector{Int}(undef, 250)\n",
    "    data = (predictions, targets)\n",
    "\n",
    "    # define sampler of predictions\n",
    "    sampler_predictions = sampler(Dirichlet(m, 0.1))\n",
    "\n",
    "    # initialize estimates\n",
    "    estimates = Vector{Float64}(undef, 100)\n",
    "\n",
    "    # for each run\n",
    "    @inbounds for i in eachindex(estimates)\n",
    "        # sample predictions\n",
    "        rand!.((sampler_predictions,), predictions)\n",
    "\n",
    "        # sample targets\n",
    "        for (j, p) in enumerate(predictions)\n",
    "            if rand() < π\n",
    "                targets[j] = 1\n",
    "            else\n",
    "                targets[j] = rand(Categorical(p))\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # evaluate estimator\n",
    "        estimates[i] = estimator(data)(predictions, targets)\n",
    "    end\n",
    "\n",
    "    return estimates\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use a helper function to run the experiment for all desired parameter settings."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct EstimatesSet\n",
    "    m::Vector{Int}\n",
    "    π::Vector{Float64}\n",
    "    estimates::Matrix{Vector{Float64}}\n",
    "end\n",
    "\n",
    "function estimates(estimator)\n",
    "    # for all combinations of m and π\n",
    "    mvec = [2, 10, 100]\n",
    "    πvec = [0.0, 0.5, 1.0]\n",
    "    estimatesmat = estimates.((estimator,), πvec', mvec)\n",
    "\n",
    "    return EstimatesSet(mvec, πvec, estimatesmat)\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "As mentioned above, we can calculate the analytic expected calibration error. For the squared\n",
    "kernel calibration error, we take the mean of the estimates of the unbiased quadratic\n",
    "estimator as approximation of the true value."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We provide simple histogram plots of our results. The mean value of the\n",
    "estimates is indicated by a solid black vertical line and the analytic\n",
    "calibration error is visualized as a dashed red line."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function plot(set::EstimatesSet; ece=false)\n",
    "    # create figure\n",
    "    f = Figure(; resolution=(1080, 960))\n",
    "\n",
    "    # add subplots\n",
    "    nrows, ncols = size(set.estimates)\n",
    "    for (j, π) in enumerate(set.π), (i, m) in enumerate(set.m)\n",
    "        # obtain data\n",
    "        estimates = set.estimates[i, j]\n",
    "\n",
    "        # create new axis\n",
    "        ax = Axis(f[i, j]; ticks=LinearTicks(4))\n",
    "        i < nrows && hidexdecorations!(current_axis(); grid=false)\n",
    "        j > 1 && hideydecorations!(current_axis(); grid=false)\n",
    "\n",
    "        # plot histogram of estimates\n",
    "        h = fit(Histogram, estimates)\n",
    "        plot!(h; color=(Dark2_8[1], 0.5), strokecolor=:black, strokewidth=0.5)\n",
    "\n",
    "        # indicate analytic calibration error for ECE\n",
    "        if ece\n",
    "            vlines!(ax, [π * (m - 1) / m]; color=Dark2_8[2], linewidth=2)\n",
    "        end\n",
    "\n",
    "        # indicate mean of estimates\n",
    "        vlines!(ax, [mean(estimates)]; color=Dark2_8[3], linewidth=2)\n",
    "    end\n",
    "\n",
    "    # add labels and link axes\n",
    "    for (j, π) in enumerate(set.π)\n",
    "        Box(f[1, j, Top()]; color=:gray90)\n",
    "        Label(f[1, j, Top()], \"π = $π\"; padding=(0, 0, 5, 5))\n",
    "        linkxaxes!(contents(f[:, j])...)\n",
    "    end\n",
    "    for (i, m) in enumerate(set.m)\n",
    "        Box(f[i, ncols, Right()]; color=:gray90)\n",
    "        Label(f[i, ncols, Right()], \"$m classes\"; rotation=-π / 2, padding=(5, 5, 0, 0))\n",
    "        linkyaxes!(contents(f[i, :])...)\n",
    "    end\n",
    "    Label(f[nrows, 1:ncols, Bottom()], \"calibration error estimate\"; padding=(0, 0, 0, 75))\n",
    "    Label(f[1:nrows, 1, Left()], \"# runs\"; rotation=π / 2, padding=(0, 75, 0, 0))\n",
    "\n",
    "    return f\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kernel choice"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use a tensor product kernel consisting of an exponential kernel\n",
    "$k(\\mu, \\mu') = \\exp{(- \\gamma \\|p - p'\\|)}$ on the space of predicted categorical\n",
    "distributions and a white kernel $k(y, y') = \\delta(y - y')$ on the space of targets\n",
    "$\\{1,\\ldots,m\\}$. The total variation distance is chosen as the norm on the space of\n",
    "predictions, and the inverse lengthscale $\\gamma$ is set according to the median\n",
    "heuristic."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "struct MedianHeuristicKernel\n",
    "    distances::Matrix{Float64}\n",
    "    cache::Vector{Float64}\n",
    "end\n",
    "\n",
    "function MedianHeuristicKernel(n::Int)\n",
    "    return MedianHeuristicKernel(\n",
    "        Matrix{Float64}(undef, n, n), Vector{Float64}(undef, (n * (n - 1)) ÷ 2)\n",
    "    )\n",
    "end\n",
    "\n",
    "function (f::MedianHeuristicKernel)((predictions, targets))\n",
    "    distances = f.distances\n",
    "    cache = f.cache\n",
    "\n",
    "    # compute inverse lengthscale with median heuristic\n",
    "    pairwise!(distances, TotalVariation(), predictions)\n",
    "    k = 0\n",
    "    @inbounds for j in axes(distances, 2), i in 1:(j - 1)\n",
    "        cache[k += 1] = distances[i, j]\n",
    "    end\n",
    "    γ = inv(median!(cache))\n",
    "\n",
    "    # create tensor product kernel\n",
    "    return transform(TVExponentialKernel(), γ) ⊗ WhiteKernel()\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Expected calibration error\n",
    "\n",
    "### Uniform binning\n",
    "\n",
    "We start by analyzing the expected calibration error (ECE).\n",
    "For our estimation we use 10 bins of uniform width in each dimension."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Random.seed!(1234)\n",
    "data = estimates(_ -> ECE(UniformBinning(10), TotalVariation()))\n",
    "plot(data; ece=true)\n",
    "save(\"./figures/ece_uniform.svg\", current_figure());"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./figures/ece_uniform.svg)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Non-uniform binning\n",
    "\n",
    "We repeat our experiments with a different data-dependent binning scheme. This\n",
    "time the bins will be computed dynamically by splitting the predictions at the\n",
    "median of the classes with the highest variance, as long as the number of bins\n",
    "does not exceed a given threshold and the number of samples per bin is above\n",
    "a certain lower bound. In our experiments we do not impose any restriction on\n",
    "the number of bins but only stop splitting if the number of samples is less\n",
    "than 10."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Random.seed!(1234)\n",
    "data = estimates(_ -> ECE(MedianVarianceBinning(10), TotalVariation()))\n",
    "plot(data; ece=true)\n",
    "save(\"./figures/ece_medianvariance.svg\", current_figure());"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./figures/ece_medianvariance.svg)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Biased estimator of the squared kernel calibration error"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Random.seed!(1234)\n",
    "data = estimates(BiasedSKCE ∘ MedianHeuristicKernel(250))\n",
    "plot(data)\n",
    "save(\"./figures/skce_biased.svg\", current_figure());"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./figures/skce_biased.svg)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Unbiased estimators of the squared kernel calibration error"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Random.seed!(1234)\n",
    "data = estimates(UnbiasedSKCE ∘ MedianHeuristicKernel(250))\n",
    "plot(data)\n",
    "save(\"./figures/skce_unbiased.svg\", current_figure());"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./figures/skce_unbiased.svg)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Random.seed!(1234)\n",
    "data = estimates(BlockUnbiasedSKCE ∘ MedianHeuristicKernel(250))\n",
    "plot(data)\n",
    "save(\"./figures/skce_blockunbiased.svg\", current_figure());"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./figures/skce_blockunbiased.svg)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.1",
   "language": "julia"
  }
 },
 "nbformat": 4
}
