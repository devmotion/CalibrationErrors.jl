<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Kernel calibration error (KCE) ¬∑ CalibrationErrors.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://devmotion.github.io/CalibrationErrors.jl/kce/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">CalibrationErrors.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">CalibrationErrors.jl</a></li><li><a class="tocitem" href="../introduction/">Introduction</a></li><li><a class="tocitem" href="../ece/">Expected calibration error (ECE)</a></li><li class="is-active"><a class="tocitem" href>Kernel calibration error (KCE)</a><ul class="internal"><li><a class="tocitem" href="#Definition"><span>Definition</span></a></li><li><a class="tocitem" href="#Estimator"><span>Estimator</span></a></li></ul></li><li><a class="tocitem" href="../others/">Other calibration errors</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/classification/">Classification of penguin species</a></li><li><a class="tocitem" href="../examples/distribution/">Distribution of calibration error estimates</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Kernel calibration error (KCE)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Kernel calibration error (KCE)</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/devmotion/CalibrationErrors.jl/blob/main/docs/src/kce.md#L" title="Edit on GitHub"><span class="docs-icon fab">ÔÇõ</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="kce"><a class="docs-heading-anchor" href="#kce">Kernel calibration error (KCE)</a><a id="kce-1"></a><a class="docs-heading-anchor-permalink" href="#kce" title="Permalink"></a></h1><h2 id="Definition"><a class="docs-heading-anchor" href="#Definition">Definition</a><a id="Definition-1"></a><a class="docs-heading-anchor-permalink" href="#Definition" title="Permalink"></a></h2><p>The kernel calibration error (KCE) is another calibration error. It is based on real-valued kernels on the product space <span>$\mathcal{P} \times \mathcal{Y}$</span> of predictions and targets.</p><p>The KCE with respect to a real-valued kernel <span>$k \colon (\mathcal{P} \times \mathcal{Y}) \times (\mathcal{P} \times \mathcal{Y}) \to \mathbb{R}$</span> is defined<sup class="footnote-reference"><a id="citeref-WLZ21" href="#footnote-WLZ21">[WLZ21]</a></sup> as</p><p class="math-container">\[\mathrm{KCE}_k := \sup_{f \in \mathcal{B}_k} \bigg| \mathbb{E}_{Y,P_X} f(P_X, Y) - \mathbb{E}_{Z_X,P_X} f(P_X, Z_X)\bigg|,\]</p><p>where <span>$\mathcal{B}_{k}$</span> is the unit ball in the <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">reproducing kernel Hilbert space (RKHS)</a> to <span>$k$</span> and <span>$Z_X$</span> is an artificial random variable on the target space <span>$\mathcal{Y}$</span> whose conditional law is given by</p><p class="math-container">\[Z_X \,|\, P_X = \mu \sim \mu.\]</p><p>The RKHS to kernel <span>$k$</span>, and hence also the unit ball <span>$\mathcal{B}_k$</span>, consists of real-valued functions of the form <span>$f \colon \mathcal{P} \times \mathcal{Y} \to \mathbb{R}$</span>.</p><p>For classification models with <span>$m$</span> classes, there exists an equivalent formulation of the KCE based on matrix-valued kernel <span>$\tilde{k} \colon \mathcal{P} \times \mathcal{P} \to \mathbb{R}^{m \times m}$</span> on the space <span>$\mathcal{P}$</span> of predictions.<sup class="footnote-reference"><a id="citeref-WLZ19" href="#footnote-WLZ19">[WLZ19]</a></sup> The definition above can be rewritten as</p><p class="math-container">\[\mathrm{KCE}_{\tilde{k}} := \sup_{f \in \mathcal{B}_{\tilde{k}}} \bigg| \mathbb{E}_{P_X} \big(\mathrm{law}(Y \,|\, P_X) - P_X\big)^\mathsf{T} f(P_X) \bigg|,\]</p><p>where the matrix-valued kernel <span>$\tilde{k}$</span> is given by</p><p class="math-container">\[\tilde{k}_{i,j}(p, q) = k((p, i), (q, j)) \quad (i,j=1,\ldots,m),\]</p><p>and <span>$\mathcal{B}_{\tilde{k}}$</span> is the unit ball in the RKHS of <span>$\tilde{k}$</span>, consisting of vector-valued functions <span>$f \colon \mathcal{P} \to \mathbb{R}^m$</span>. However, this formulation applies only to classification models whereas the general definition above covers all probabilistic predictive models.</p><p>For a large class of kernels the KCE is zero if and only if the model is calibrated.<sup class="footnote-reference"><a id="citeref-WLZ21" href="#footnote-WLZ21">[WLZ21]</a></sup> Moreover, the squared KCE (SKCE) can be formulated in terms of the kernel <span>$k$</span> as</p><p class="math-container">\[\begin{aligned}
\mathrm{SKCE}_{k} := \mathrm{KCE}_k^2 &amp;= \int k(u, v) \, \big(\mathrm{law}(P_X, Y) - \mathrm{law}(P_X, Z_X)\big)(u) \big(\mathrm{law}(P_X, Y) - \mathrm{law}(P_X, Z_X)\big)(v) \\
&amp;= \mathbb{E} h_k\big((P_X, Y), (P_{X&#39;}, Y&#39;)\big),
\end{aligned}\]</p><p>where <span>$(X&#39;,Y&#39;)$</span> is an independent copy of <span>$(X,Y)$</span> and</p><p class="math-container">\[\begin{aligned}
h_k\big((\mu, y), (\mu&#39;, y&#39;)\big) :={}&amp; k\big((\mu, y), (\mu&#39;, y&#39;)\big) - \mathbb{E}_{Z \sim \mu} k\big((\mu, Z), (\mu&#39;, y&#39;)\big) \\
&amp;- \mathbb{E}_{Z&#39; \sim \mu&#39;} k\big((\mu, y), (\mu&#39;, Z&#39;)\big) + \mathbb{E}_{Z \sim \mu, Z&#39; \sim \mu&#39;} k\big((\mu, Z), (\mu&#39;, Z&#39;)\big).
\end{aligned}\]</p><p>The KCE is actually a special case of calibration errors that are formulated as integral probability metrics of the form</p><p class="math-container">\[\sup_{f \in \mathcal{F}} \big| \mathbb{E}_{Y,P_X} f(P_X, Y) - \mathbb{E}_{Z_X,P_X} f(P_X, Z_X)\big|,\]</p><p>where <span>$\mathcal{F}$</span> is a space of real-valued functions of the form <span>$f \colon \mathcal{P} \times \mathcal{Y} \to \mathbb{R}$</span>.<sup class="footnote-reference"><a id="citeref-WLZ21" href="#footnote-WLZ21">[WLZ21]</a></sup> For classification models, the <a href="../ece/#ece">ECE</a> with respect to common distances such as the total variation distance or the squared Euclidean distance can be formulated in this way.<sup class="footnote-reference"><a id="citeref-WLZ19" href="#footnote-WLZ19">[WLZ19]</a></sup></p><p>The maximum mean calibration error (MMCE)<sup class="footnote-reference"><a id="citeref-KSJ" href="#footnote-KSJ">[KSJ]</a></sup> can be viewed as a special case of the KCE, in which only the most-confident predictions are considered.<sup class="footnote-reference"><a id="citeref-WLZ19" href="#footnote-WLZ19">[WLZ19]</a></sup></p><h2 id="Estimator"><a class="docs-heading-anchor" href="#Estimator">Estimator</a><a id="Estimator-1"></a><a class="docs-heading-anchor-permalink" href="#Estimator" title="Permalink"></a></h2><p>For the SKCE biased and unbiased estimators exist. In CalibrationErrors.jl <a href="#CalibrationErrors.SKCE"><code>SKCE</code></a> lets you construct unbiased and biased estimators with quadratic and sub-quadratic sample complexity.</p><article class="docstring"><header><a class="docstring-binding" id="CalibrationErrors.SKCE" href="#CalibrationErrors.SKCE"><code>CalibrationErrors.SKCE</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SKCE(k; unbiased::Bool=true, blocksize=identity)</code></pre><p>Estimator of the squared kernel calibration error (SKCE) with kernel <code>k</code>.</p><p>Kernel <code>k</code> on the product space of predictions and targets has to be a <code>Kernel</code> from the Julia package <a href="https://github.com/JuliaGaussianProcesses/KernelFunctions.jl">KernelFunctions.jl</a> that can be evaluated for inputs that are tuples of predictions and targets.</p><p>One can choose an unbiased or a biased variant with <code>unbiased=true</code> or <code>unbiased=false</code>, respectively (see details below).</p><p>The SKCE is estimated as the average estimate of different blocks of samples. The number of samples per block is set by <code>blocksize</code>:</p><ul><li>If <code>blocksize</code> is a function <code>blocksize(n::Int)</code>, then the number of samples per block is set to <code>blocksize(n)</code> where <code>n</code> is the total number of samples.</li><li>If <code>blocksize</code> is an integer, then the number of samplers per block is set to <code>blocksize</code>, indepedent of the total number of samples.</li></ul><p>The default setting <code>blocksize=identity</code> implies that a single block with all samples is used.</p><p>The number of samples per block must be at least 1 if <code>unbiased=false</code> and 2 if <code>unbiased=true</code>. Additionally, it must be at most the total number of samples. Note that the last block is neglected if it is incomplete (see details below).</p><p><strong>Details</strong></p><p>The unbiased estimator is not guaranteed to be non-negative whereas the biased estimator is always non-negative.</p><p>The sample complexity of the estimator is <span>$O(mn)$</span>, where <span>$m$</span> is the block size and <span>$n$</span> is the total number of samples. In particular, with the default setting <code>blocksize=identity</code> the estimator has a quadratic sample complexity.</p><p>Let <span>$(P_{X_i}, Y_i)_{i=1,\ldots,n}$</span> be a data set of predictions and corresponding targets. The estimator with block size <span>$m$</span> is defined as</p><p class="math-container">\[{\bigg\lfloor \frac{n}{m} \bigg\rfloor}^{-1} \sum_{b=1}^{\lfloor n/m \rfloor}
|B_b|^{-1} \sum_{(i, j) \in B_b} h_k\big((P_{X_i}, Y_i), (P_{X_j}, Y_j)\big),\]</p><p>where</p><p class="math-container">\[\begin{aligned}
h_k\big((Œº, y), (Œº&#39;, y&#39;)\big) ={}&amp;   k\big((Œº, y), (Œº&#39;, y&#39;)\big)
                                   - ùîº_{Z ‚àº Œº} k\big((Œº, Z), (Œº&#39;, y&#39;)\big) \\
                                 &amp; - ùîº_{Z&#39; ‚àº Œº&#39;} k\big((Œº, y), (Œº&#39;, Z&#39;)\big)
                                   + ùîº_{Z ‚àº Œº, Z&#39; ‚àº Œº&#39;} k\big((Œº, Z), (Œº&#39;, Z&#39;)\big)
\end{aligned}\]</p><p>and blocks <span>$B_b$</span> (<span>$b = 1, \ldots, \lfloor n/m \rfloor$</span>) are defined as</p><p class="math-container">\[B_b = \begin{cases}
\{(i, j): (b - 1) m &lt; i &lt; j \leq bm \} &amp; \text{(unbiased)}, \\
\{(i, j): (b - 1) m &lt; i, j \leq bm \} &amp; \text{(biased)}.
\end{cases}\]</p><p><strong>References</strong></p><p>Widmann, D., Lindsten, F., &amp; Zachariah, D. (2019). <a href="https://proceedings.neurips.cc/paper/2019/hash/1c336b8080f82bcc2cd2499b4c57261d-Abstract.html">Calibration tests in multi-class classification: A unifying framework</a>. In: Advances in Neural Information Processing Systems (NeurIPS 2019) (pp. 12257‚Äì12267).</p><p>Widmann, D., Lindsten, F., &amp; Zachariah, D. (2021). <a href="https://openreview.net/forum?id=-bxf89v3Nx">Calibration tests beyond classification</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/devmotion/CalibrationErrors.jl/blob/ad89fba22cc1c0604d142a11b54cde3ac6b6c578/src/skce.jl#LL1-L67">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-KSJ"><a class="tag is-link" href="#citeref-KSJ">KSJ</a>Kumar, A., Sarawagi, S., &amp; Jain, U. (2018). <a href="http://proceedings.mlr.press/v80/kumar18a.html">Trainable calibration measures for neural networks from kernel mean embeddings</a>. In <em>Proceedings of the 35th International Conference on Machine Learning</em> (pp. 2805-2814).</li><li class="footnote" id="footnote-WLZ19"><a class="tag is-link" href="#citeref-WLZ19">WLZ19</a>Widmann, D., Lindsten, F., &amp; Zachariah, D. (2019). <a href="https://proceedings.neurips.cc/paper/2019/hash/1c336b8080f82bcc2cd2499b4c57261d-Abstract.html">Calibration tests in multi-class classification: A unifying framework</a>. In <em>Advances in Neural Information Processing Systems 32 (NeurIPS 2019)</em> (pp. 12257‚Äì12267).</li><li class="footnote" id="footnote-WLZ21"><a class="tag is-link" href="#citeref-WLZ21">WLZ21</a>Widmann, D., Lindsten, F., &amp; Zachariah, D. (2021). <a href="https://openreview.net/forum?id=-bxf89v3Nx">Calibration tests beyond classification</a>. To be presented at <em>ICLR 2021</em>.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../ece/">¬´ Expected calibration error (ECE)</a><a class="docs-footer-nextpage" href="../others/">Other calibration errors ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Sunday 30 October 2022 21:19">Sunday 30 October 2022</span>. Using Julia version 1.8.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
