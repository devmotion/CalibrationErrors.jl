@doc raw"""
    SKCE(k; unbiased::Bool=true, blocksize=identity)

Estimator of the squared kernel calibration error (SKCE) with kernel `k`.

Kernel `k` on the product space of predictions and targets has to be a `Kernel` from the
Julia package
[KernelFunctions.jl](https://github.com/JuliaGaussianProcesses/KernelFunctions.jl)
that can be evaluated for inputs that are tuples of predictions and targets.

One can choose an unbiased or a biased variant with `unbiased=true` or `unbiased=false`,
respectively (see details below).

The SKCE is estimated as the average estimate of different blocks of samples. The number of
samples per block is set by `blocksize`:
- If `blocksize` is a function `blocksize(n::Int)`, then the number of samples per block is
  set to `blocksize(n)` where `n` is the total number of samples.
- If `blocksize` is an integer, then the number of samplers per block is set to `blocksize`,
  indepedent of the total number of samples.
The default setting `blocksize=identity` implies that a single block with all samples is
used.

The number of samples per block must be at least 1 if `unbiased=false` and 2 if
`unbiased=true`. Additionally, it must be at most the total number of samples. Note that the
last block is neglected if it is incomplete (see details below).

# Details

The unbiased estimator is not guaranteed to be non-negative whereas the biased estimator is
always non-negative.

The sample complexity of the estimator is ``O(mn)``, where ``m`` is the block size and ``n``
is the total number of samples. In particular, with the default setting `blocksize=identity`
the estimator has a quadratic sample complexity.

Let ``(P_{X_i}, Y_i)_{i=1,\ldots,n}`` be a data set of predictions and corresponding
targets. The estimator with block size ``m`` is defined as
```math
{\bigg\lfloor \frac{n}{m} \bigg\rfloor}^{-1} \sum_{b=1}^{\lfloor n/m \rfloor}
|B_b|^{-1} \sum_{(i, j) \in B_b} h_k\big((P_{X_i}, Y_i), (P_{X_j}, Y_j)\big),
```
where
```math
\begin{aligned}
h_k\big((Î¼, y), (Î¼', y')\big) ={}&   k\big((Î¼, y), (Î¼', y')\big)
                                   - ğ”¼_{Z âˆ¼ Î¼} k\big((Î¼, Z), (Î¼', y')\big) \\
                                 & - ğ”¼_{Z' âˆ¼ Î¼'} k\big((Î¼, y), (Î¼', Z')\big)
                                   + ğ”¼_{Z âˆ¼ Î¼, Z' âˆ¼ Î¼'} k\big((Î¼, Z), (Î¼', Z')\big)
\end{aligned}
```
and blocks ``B_b`` (``b = 1, \ldots, \lfloor n/m \rfloor``) are defined as
```math
B_b = \begin{cases}
\{(i, j): (b - 1) m < i < j \leq bm \} & \text{(unbiased)}, \\
\{(i, j): (b - 1) m < i, j \leq bm \} & \text{(biased)}.
\end{cases}
```

# References

Widmann, D., Lindsten, F., & Zachariah, D. (2019). [Calibration tests in multi-class
classification: A unifying framework](https://proceedings.neurips.cc/paper/2019/hash/1c336b8080f82bcc2cd2499b4c57261d-Abstract.html).
In: Advances in Neural Information Processing Systems (NeurIPS 2019) (pp. 12257â€“12267).

Widmann, D., Lindsten, F., & Zachariah, D. (2021). [Calibration tests beyond
classification](https://openreview.net/forum?id=-bxf89v3Nx).
"""
struct SKCE{K<:Kernel,B} <: CalibrationErrorEstimator
    """Kernel of estimator."""
    kernel::K
    """Whether the unbiased estimator is used."""
    unbiased::Bool
    """Number of samples per block."""
    blocksize::B

    function SKCE{K,B}(kernel::K, unbiased::Bool, blocksize::B) where {K,B}
        if blocksize isa Integer
            blocksize â‰¥ 1 + unbiased || throw(
                ArgumentError(
                    "there must be at least $(1 + unbiased) $(unbiased ? "samples" : "sample") per block",
                ),
            )
        end
        return new{K,B}(kernel, unbiased, blocksize)
    end
end

function SKCE(kernel::Kernel; unbiased::Bool=true, blocksize::B=identity) where {B}
    return SKCE{typeof(kernel),B}(kernel, unbiased, blocksize)
end

## estimators without blocks
function (skce::SKCE{<:Kernel,typeof(identity)})(
    predictions::AbstractVector, targets::AbstractVector
)
    @unpack kernel, unbiased = skce
    return if unbiased
        unbiasedskce(kernel, predictions, targets)
    else
        biasedskce(kernel, predictions, targets)
    end
end

### unbiased estimator (no blocks)
function unbiasedskce(kernel::Kernel, predictions::AbstractVector, targets::AbstractVector)
    # obtain number of samples
    nsamples = check_nsamples(predictions, targets, 2)

    @inbounds begin
        # evaluate the kernel function for the first pair of samples
        hij = unsafe_skce_eval(
            kernel, predictions[1], targets[1], predictions[2], targets[2]
        )

        # initialize the estimate
        estimate = hij / 1

        # for all other pairs of samples
        n = 1
        for j in 3:nsamples
            predictionj = predictions[j]
            targetj = targets[j]

            for i in 1:(j - 1)
                predictioni = predictions[i]
                targeti = targets[i]

                # evaluate the kernel function
                hij = unsafe_skce_eval(kernel, predictioni, targeti, predictionj, targetj)

                # update the estimate
                n += 1
                estimate += (hij - estimate) / n
            end
        end
    end

    return estimate
end

### biased estimator (no blocks)
function biasedskce(kernel::Kernel, predictions::AbstractVector, targets::AbstractVector)
    # obtain number of samples
    nsamples = check_nsamples(predictions, targets, 1)

    @inbounds begin
        # evaluate kernel function for the first sample
        prediction = predictions[1]
        target = targets[1]
        hij = unsafe_skce_eval(kernel, prediction, target, prediction, target)

        # initialize the calibration error estimate
        estimate = hij / 1

        # for all other pairs of samples
        n = 1
        for i in 2:nsamples
            predictioni = predictions[i]
            targeti = targets[i]

            for j in 1:(i - 1)
                predictionj = predictions[j]
                targetj = targets[j]

                # evaluate the kernel function
                hij = unsafe_skce_eval(kernel, predictioni, targeti, predictionj, targetj)

                # update the estimate (add two terms due to symmetry!)
                n += 2
                estimate += 2 * (hij - estimate) / n
            end

            # evaluate the kernel function
            hij = unsafe_skce_eval(kernel, predictioni, targeti, predictioni, targeti)

            # update the estimate
            n += 1
            estimate += (hij - estimate) / n
        end
    end

    return estimate
end

## estimators with blocks
function (skce::SKCE)(predictions::AbstractVector, targets::AbstractVector)
    @unpack kernel, unbiased, blocksize = skce

    # obtain number of samples
    nsamples = check_nsamples(predictions, targets, 1 + unbiased)

    # compute number of blocks
    _blocksize = blocksize isa Integer ? blocksize : blocksize(nsamples)
    (_blocksize isa Integer && _blocksize >= 1 + unbiased) ||
        error("number of samples per block must be an integer >= $(1 + unbiased)")
    nblocks = nsamples Ã· _blocksize
    nblocks >= 1 || error("at least one block of samples is required")

    # create iterator of partitions
    blocks = Iterators.take(
        zip(
            Iterators.partition(predictions, _blocksize),
            Iterators.partition(targets, _blocksize),
        ),
        nblocks,
    )

    # compute average estimate
    estimator = SKCE(kernel; unbiased=unbiased)
    estimate = mean(
        estimator(_predictions, _targets) for (_predictions, _targets) in blocks
    )

    return estimate
end

"""
    unsafe_skce_eval(k, p, y, pÌƒ, yÌƒ)

Evaluate
```math
k((p, y), (pÌƒ, yÌƒ)) - E_{z âˆ¼ p}[k((p, z), (pÌƒ, yÌƒ))] - E_{zÌƒ âˆ¼ pÌƒ}[k((p, y), (pÌƒ, zÌƒ))] + E_{z âˆ¼ p, zÌƒ âˆ¼ pÌƒ}[k((p, z), (pÌƒ, zÌƒ))]
```
for kernel `k` and predictions `p` and `pÌƒ` with corresponding targets `y` and `yÌƒ`.

This method assumes that `p`, `pÌƒ`, `y`, and `yÌƒ` are valid and specified correctly, and
does not perform any checks.
"""
function unsafe_skce_eval end

# default implementation for classification
# we do not use the symmetry of `kernel` since it seems unlikely that `(p, y) == (pÌƒ, yÌƒ)`
function unsafe_skce_eval(
    kernel::Kernel,
    p::AbstractVector{<:Real},
    y::Integer,
    pÌƒ::AbstractVector{<:Real},
    yÌƒ::Integer,
)
    # precomputations
    n = length(p)

    @inbounds py = p[y]
    @inbounds pÌƒyÌƒ = pÌƒ[yÌƒ]
    pym1 = py - 1
    pÌƒyÌƒm1 = pÌƒyÌƒ - 1

    tuple_p_y = (p, y)
    tuple_pÌƒ_yÌƒ = (pÌƒ, yÌƒ)

    # i = y, j = yÌƒ
    result = kernel((p, y), (pÌƒ, yÌƒ)) * (1 - py - pÌƒyÌƒ + py * pÌƒyÌƒ)

    # i < y
    for i in 1:(y - 1)
        @inbounds pi = p[i]
        tuple_p_i = (p, i)

        # j < yÌƒ
        @inbounds for j in 1:(yÌƒ - 1)
            result += kernel(tuple_p_i, (pÌƒ, j)) * pi * pÌƒ[j]
        end

        # j = yÌƒ
        result += kernel(tuple_p_i, tuple_pÌƒ_yÌƒ) * pi * pÌƒyÌƒm1

        # j > yÌƒ
        @inbounds for j in (yÌƒ + 1):n
            result += kernel(tuple_p_i, (pÌƒ, j)) * pi * pÌƒ[j]
        end
    end

    # i = y, j < yÌƒ
    @inbounds for j in 1:(yÌƒ - 1)
        result += kernel(tuple_p_y, (pÌƒ, j)) * pym1 * pÌƒ[j]
    end

    # i = y, j > yÌƒ
    @inbounds for j in (yÌƒ + 1):n
        result += kernel(tuple_p_y, (pÌƒ, j)) * pym1 * pÌƒ[j]
    end

    # i > y
    for i in (y + 1):n
        @inbounds pi = p[i]
        tuple_p_i = (p, i)

        # j < yÌƒ
        @inbounds for j in 1:(yÌƒ - 1)
            result += kernel(tuple_p_i, (pÌƒ, j)) * pi * pÌƒ[j]
        end

        # j = yÌƒ
        result += kernel(tuple_p_i, tuple_pÌƒ_yÌƒ) * pi * pÌƒyÌƒm1

        # j > yÌƒ
        @inbounds for j in (yÌƒ + 1):n
            result += kernel(tuple_p_i, (pÌƒ, j)) * pi * pÌƒ[j]
        end
    end

    return result
end

# for binary classification with probabilities (corresponding to parameters of Bernoulli
# distributions) and boolean targets the expression simplifies to
# ```math
# k((p, y), (pÌƒ, yÌƒ)) = (y(1-p) + (1-y)p)(yÌƒ(1-pÌƒ) + (1-yÌƒ)pÌƒ)(k((p, y), (pÌƒ, yÌƒ)) - k((p, 1-y), (pÌƒ, yÌƒ)) - k((p, y), (pÌƒ, 1-yÌƒ)) + k((p, 1-y), (pÌƒ, 1-yÌƒ)))
# ```
function unsafe_skce_eval(kernel::Kernel, p::Real, y::Bool, pÌƒ::Real, yÌƒ::Bool)
    noty = !y
    notyÌƒ = !yÌƒ
    z =
        kernel((p, y), (pÌƒ, yÌƒ)) - kernel((p, noty), (pÌƒ, yÌƒ)) -
        kernel((p, y), (pÌƒ, notyÌƒ)) + kernel((p, noty), (pÌƒ, notyÌƒ))
    return (y ? 1 - p : p) * (yÌƒ ? 1 - pÌƒ : pÌƒ) * z
end

# evaluation for tensor product kernels
function unsafe_skce_eval(kernel::KernelTensorProduct, p, y, pÌƒ, yÌƒ)
    Îºpredictions, Îºtargets = kernel.kernels
    return Îºpredictions(p, pÌƒ) * unsafe_skce_eval_targets(Îºtargets, p, y, pÌƒ, yÌƒ)
end

# resolve method ambiguity
function unsafe_skce_eval(
    kernel::KernelTensorProduct,
    p::AbstractVector{<:Real},
    y::Integer,
    pÌƒ::AbstractVector{<:Real},
    yÌƒ::Integer,
)
    Îºpredictions, Îºtargets = kernel.kernels
    return Îºpredictions(p, pÌƒ) * unsafe_skce_eval_targets(Îºtargets, p, y, pÌƒ, yÌƒ)
end
function unsafe_skce_eval(kernel::KernelTensorProduct, p::Real, y::Bool, pÌƒ::Real, yÌƒ::Bool)
    Îºpredictions, Îºtargets = kernel.kernels
    return Îºpredictions(p, pÌƒ) * unsafe_skce_eval_targets(Îºtargets, p, y, pÌƒ, yÌƒ)
end

function unsafe_skce_eval_targets(
    Îºtargets::Kernel,
    p::AbstractVector{<:Real},
    y::Integer,
    pÌƒ::AbstractVector{<:Real},
    yÌƒ::Integer,
)
    # ensure that y â‰¤ yÌƒ (simplifies the implementation)
    y > yÌƒ && return unsafe_skce_eval_targets(Îºtargets, pÌƒ, yÌƒ, p, y)

    # precomputations
    n = length(p)

    @inbounds begin
        py = p[y]
        pyÌƒ = p[yÌƒ]
        pÌƒy = pÌƒ[y]
        pÌƒyÌƒ = pÌƒ[yÌƒ]
    end
    pym1 = py - 1
    pyÌƒm1 = pyÌƒ - 1
    pÌƒym1 = pÌƒy - 1
    pÌƒyÌƒm1 = pÌƒyÌƒ - 1

    # i = y, j = yÌƒ
    result = Îºtargets(y, yÌƒ) * (1 - py - pÌƒyÌƒ + py * pÌƒyÌƒ)

    # i < y
    for i in 1:(y - 1)
        @inbounds pi = p[i]
        @inbounds pÌƒi = pÌƒ[i]

        # i = j < y â‰¤ yÌƒ
        result += Îºtargets(i, i) * pi * pÌƒi

        # i < j < y â‰¤ yÌƒ
        @inbounds for j in (i + 1):(y - 1)
            result += Îºtargets(i, j) * (pi * pÌƒ[j] + p[j] * pÌƒi)
        end

        # i < y < j < yÌƒ
        @inbounds for j in (y + 1):(yÌƒ - 1)
            result += Îºtargets(i, j) * (pi * pÌƒ[j] + p[j] * pÌƒi)
        end

        # i < y â‰¤ yÌƒ < j
        @inbounds for j in (yÌƒ + 1):n
            result += Îºtargets(i, j) * (pi * pÌƒ[j] + p[j] * pÌƒi)
        end
    end

    # y < i < yÌƒ
    for i in (y + 1):(yÌƒ - 1)
        @inbounds pi = p[i]
        @inbounds pÌƒi = pÌƒ[i]

        # y < i = j < yÌƒ
        result += Îºtargets(i, i) * pi * pÌƒi

        # y < i < j < yÌƒ
        @inbounds for j in (i + 1):(yÌƒ - 1)
            result += Îºtargets(i, j) * (pi * pÌƒ[j] + p[j] * pÌƒi)
        end

        # y < i < yÌƒ < j
        @inbounds for j in (yÌƒ + 1):n
            result += Îºtargets(i, j) * (pi * pÌƒ[j] + p[j] * pÌƒi)
        end
    end

    # yÌƒ < i
    for i in (yÌƒ + 1):n
        @inbounds pi = p[i]
        @inbounds pÌƒi = pÌƒ[i]

        # yÌƒ < i = j
        result += Îºtargets(i, i) * pi * pÌƒi

        # yÌƒ < i < j
        @inbounds for j in (i + 1):n
            result += Îºtargets(i, j) * (pi * pÌƒ[j] + p[j] * pÌƒi)
        end
    end

    # handle special case y = yÌƒ
    if y == yÌƒ
        # i < y = yÌƒ, j = y = yÌƒ
        @inbounds for i in 1:(y - 1)
            result += Îºtargets(i, y) * (p[i] * pÌƒym1 + pym1 * pÌƒ[i])
        end

        # i = y = yÌƒ, j > y = yÌƒ
        @inbounds for j in (y + 1):n
            result += Îºtargets(y, j) * (pym1 * pÌƒ[j] + p[j] * pÌƒym1)
        end
    else
        # i < y
        for i in 1:(y - 1)
            @inbounds pi = p[i]
            @inbounds pÌƒi = pÌƒ[i]

            # j = y < yÌƒ
            result += Îºtargets(i, y) * (pi * pÌƒy + pym1 * pÌƒi)

            # y < j = yÌƒ
            result += Îºtargets(i, yÌƒ) * (pi * pÌƒyÌƒm1 + pyÌƒ * pÌƒi)
        end

        # i = y = j < yÌƒ
        result += Îºtargets(y, y) * pym1 * pÌƒy

        # i = y < j < yÌƒ and y < i < j = yÌƒ
        for ij in (y + 1):(yÌƒ - 1)
            @inbounds pij = p[ij]
            @inbounds pÌƒij = pÌƒ[ij]

            # i = y < j < yÌƒ
            result += Îºtargets(y, ij) * (pym1 * pÌƒij + pij * pÌƒy)

            # y < i < j = yÌƒ
            result += Îºtargets(ij, yÌƒ) * (pij * pÌƒyÌƒm1 + pyÌƒ * pÌƒij)
        end

        # i = yÌƒ = j
        result += Îºtargets(yÌƒ, yÌƒ) * pyÌƒ * (pÌƒyÌƒ - 1)

        # i = y < yÌƒ < j and i = yÌƒ < j
        for j in (yÌƒ + 1):n
            @inbounds pj = p[j]
            @inbounds pÌƒj = pÌƒ[j]

            # i = y < yÌƒ < j
            result += Îºtargets(y, j) * (pym1 * pÌƒj + pj * pÌƒy)

            # i = yÌƒ < j
            result += Îºtargets(yÌƒ, j) * (pÌƒyÌƒm1 * pj + pÌƒj * pyÌƒ)
        end
    end

    return result
end

function unsafe_skce_eval_targets(
    ::WhiteKernel,
    p::AbstractVector{<:Real},
    y::Integer,
    pÌƒ::AbstractVector{<:Real},
    yÌƒ::Integer,
)
    @inbounds res = (y == yÌƒ) - p[yÌƒ] - pÌƒ[y] + dot(p, pÌƒ)
    return res
end

function unsafe_skce_eval_targets(Îºtargets::Kernel, p::Real, y::Bool, pÌƒ::Real, yÌƒ::Bool)
    noty = !y
    notyÌƒ = !yÌƒ
    z = Îºtargets(y, yÌƒ) - Îºtargets(noty, yÌƒ) - Îºtargets(y, notyÌƒ) + Îºtargets(noty, notyÌƒ)
    return (y ? 1 - p : p) * (yÌƒ ? 1 - pÌƒ : pÌƒ) * z
end
function unsafe_skce_eval_targets(::WhiteKernel, p::Real, y::Bool, pÌƒ::Real, yÌƒ::Bool)
    return 2 * (y - p) * (yÌƒ - pÌƒ)
end
