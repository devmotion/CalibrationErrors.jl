<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Kernel calibration error (KCE) ¬∑ CalibrationErrors.jl</title><link rel="canonical" href="https://devmotion.github.io/CalibrationErrors.jl/kce/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">CalibrationErrors.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">CalibrationErrors.jl</a></li><li><a class="tocitem" href="../introduction/">Introduction</a></li><li><a class="tocitem" href="../ece/">Expected calibration error (ECE)</a></li><li class="is-active"><a class="tocitem" href>Kernel calibration error (KCE)</a><ul class="internal"><li><a class="tocitem" href="#Definition"><span>Definition</span></a></li><li><a class="tocitem" href="#Estimators"><span>Estimators</span></a></li></ul></li><li><a class="tocitem" href="../others/">Other calibration errors</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/classification/">Classification of penguin species</a></li><li><a class="tocitem" href="../examples/distribution/">Distribution of calibration error estimates</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Kernel calibration error (KCE)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Kernel calibration error (KCE)</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/devmotion/CalibrationErrors.jl/blob/master/docs/src/kce.md#L" title="Edit on GitHub"><span class="docs-icon fab">ÔÇõ</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="kce"><a class="docs-heading-anchor" href="#kce">Kernel calibration error (KCE)</a><a id="kce-1"></a><a class="docs-heading-anchor-permalink" href="#kce" title="Permalink"></a></h1><h2 id="Definition"><a class="docs-heading-anchor" href="#Definition">Definition</a><a id="Definition-1"></a><a class="docs-heading-anchor-permalink" href="#Definition" title="Permalink"></a></h2><p>The kernel calibration error (KCE) is another calibration error. It is based on real-valued kernels on the product space <span>$\mathcal{P} \times \mathcal{Y}$</span> of predictions and targets.</p><p>The KCE with respect to a real-valued kernel <span>$k \colon (\mathcal{P} \times \mathcal{Y}) \times (\mathcal{P} \times \mathcal{Y}) \to \mathbb{R}$</span> is defined<sup class="footnote-reference"><a id="citeref-WLZ21" href="#footnote-WLZ21">[WLZ21]</a></sup> as</p><p class="math-container">\[\mathrm{KCE}_k := \sup_{f \in \mathcal{B}_k} \bigg| \mathbb{E}_{Y,P_X} f(P_X, Y) - \mathbb{E}_{Z_X,P_X} f(P_X, Z_X)\bigg|,\]</p><p>where <span>$\mathcal{B}_{k}$</span> is the unit ball in the <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">reproducing kernel Hilbert space (RKHS)</a> to <span>$k$</span> and <span>$Z_X$</span> is an artificial random variable on the target space <span>$\mathcal{Y}$</span> whose conditional law is given by</p><p class="math-container">\[Z_X \,|\, P_X = \mu \sim \mu.\]</p><p>The RKHS to kernel <span>$k$</span>, and hence also the unit ball <span>$\mathcal{B}_k$</span>, consists of real-valued functions of the form <span>$f \colon \mathcal{P} \times \mathcal{Y} \to \mathbb{R}$</span>.</p><p>For classification models with <span>$m$</span> classes, there exists an equivalent formulation of the KCE based on matrix-valued kernel <span>$\tilde{k} \colon \mathcal{P} \times \mathcal{P} \to \mathbb{R}^{m \times m}$</span> on the space <span>$\mathcal{P}$</span> of predictions.<sup class="footnote-reference"><a id="citeref-WLZ19" href="#footnote-WLZ19">[WLZ19]</a></sup> The definition above can be rewritten as</p><p class="math-container">\[\mathrm{KCE}_{\tilde{k}} := \sup_{f \in \mathcal{B}_{\tilde{k}}} \bigg| \mathbb{E}_{P_X} \big(\mathrm{law}(Y \,|\, P_X) - P_X\big)^\mathsf{T} f(P_X) \bigg|,\]</p><p>where the matrix-valued kernel <span>$\tilde{k}$</span> is given by</p><p class="math-container">\[\tilde{k}_{i,j}(p, q) = k((p, i), (q, j)) \quad (i,j=1,\ldots,m),\]</p><p>and <span>$\mathcal{B}_{\tilde{k}}$</span> is the unit ball in the RKHS of <span>$\tilde{k}$</span>, consisting of vector-valued functions <span>$f \colon \mathcal{P} \to \mathbb{R}^m$</span>. However, this formulation applies only to classification models whereas the general definition above covers all probabilistic predictive models.</p><p>For a large class of kernels the KCE is zero if and only if the model is calibrated.<sup class="footnote-reference"><a id="citeref-WLZ21" href="#footnote-WLZ21">[WLZ21]</a></sup> Moreover, the squared KCE (SKCE) can be formulated in terms of the kernel <span>$k$</span> as</p><p class="math-container">\[\begin{aligned}
\mathrm{SKCE}_{k} := \mathrm{KCE}_k^2 &amp;= \int k(u, v) \, \big(\mathrm{law}(P_X, Y) - \mathrm{law}(P_X, Z_X)\big)(u) \big(\mathrm{law}(P_X, Y) - \mathrm{law}(P_X, Z_X)\big)(v) \\
&amp;= \mathbb{E} h_k\big((P_X, Y), (P_{X&#39;}, Y&#39;)\big),
\end{aligned}\]</p><p>where <span>$(X&#39;,Y&#39;)$</span> is an independent copy of <span>$(X,Y)$</span> and</p><p class="math-container">\[\begin{aligned}
h_k\big((\mu, y), (\mu&#39;, y&#39;)\big) :={}&amp; k\big((\mu, y), (\mu&#39;, y&#39;)\big) - \mathbb{E}_{Z \sim \mu} k\big((\mu, Z), (\mu&#39;, y&#39;)\big) \\
&amp;- \mathbb{E}_{Z&#39; \sim \mu&#39;} k\big((\mu, y), (\mu&#39;, Z&#39;)\big) + \mathbb{E}_{Z \sim \mu, Z&#39; \sim \mu&#39;} k\big((\mu, Z), (\mu&#39;, Z&#39;)\big).
\end{aligned}\]</p><p>The KCE is actually a special case of calibration errors that are formulated as integral probability metrics of the form</p><p class="math-container">\[\sup_{f \in \mathcal{F}} \big| \mathbb{E}_{Y,P_X} f(P_X, Y) - \mathbb{E}_{Z_X,P_X} f(P_X, Z_X)\big|,\]</p><p>where <span>$\mathcal{F}$</span> is a space of real-valued functions of the form <span>$f \colon \mathcal{P} \times \mathcal{Y} \to \mathbb{R}$</span>.<sup class="footnote-reference"><a id="citeref-WLZ21" href="#footnote-WLZ21">[WLZ21]</a></sup> For classification models, the <a href="../ece/#ece">ECE</a> with respect to common distances such as the total variation distance or the squared Euclidean distance can be formulated in this way.<sup class="footnote-reference"><a id="citeref-WLZ19" href="#footnote-WLZ19">[WLZ19]</a></sup></p><p>The maximum mean calibration error (MMCE)<sup class="footnote-reference"><a id="citeref-KSJ" href="#footnote-KSJ">[KSJ]</a></sup> can be viewed as a special case of the KCE, in which only the most-confident predictions are considered.<sup class="footnote-reference"><a id="citeref-WLZ19" href="#footnote-WLZ19">[WLZ19]</a></sup></p><h2 id="Estimators"><a class="docs-heading-anchor" href="#Estimators">Estimators</a><a id="Estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Estimators" title="Permalink"></a></h2><p>For the SKCE biased and unbiased estimators exist. In CalibrationErrors.jl three types of estimators are available, namely <a href="#CalibrationErrors.BiasedSKCE"><code>BiasedSKCE</code></a>, <a href="#CalibrationErrors.UnbiasedSKCE"><code>UnbiasedSKCE</code></a>, and <a href="#CalibrationErrors.BlockUnbiasedSKCE"><code>BlockUnbiasedSKCE</code></a>. Unsurprisingly, <a href="#CalibrationErrors.BiasedSKCE"><code>BiasedSKCE</code></a> is a biased estimator whereas the other two estimators are unbiased. <a href="#CalibrationErrors.BiasedSKCE"><code>BiasedSKCE</code></a> and <a href="#CalibrationErrors.UnbiasedSKCE"><code>UnbiasedSKCE</code></a> have quadratic sample complexity whereas <a href="#CalibrationErrors.BlockUnbiasedSKCE"><code>BlockUnbiasedSKCE</code></a> is an estimator with linear sample complexity.</p><h3 id="Biased-estimator"><a class="docs-heading-anchor" href="#Biased-estimator">Biased estimator</a><a id="Biased-estimator-1"></a><a class="docs-heading-anchor-permalink" href="#Biased-estimator" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CalibrationErrors.BiasedSKCE" href="#CalibrationErrors.BiasedSKCE"><code>CalibrationErrors.BiasedSKCE</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">BiasedSKCE(k)</code></pre><p>Biased estimator of the squared kernel calibration error (SKCE) with kernel <code>k</code>.</p><p>Kernel <code>k</code> on the product space of predictions and targets has to be a <code>Kernel</code> from the Julia package <a href="https://github.com/JuliaGaussianProcesses/KernelFunctions.jl">KernelFunctions.jl</a> that can be evaluated for inputs that are tuples of predictions and targets.</p><p><strong>Details</strong></p><p>The estimator is biased and guaranteed to be non-negative. Its sample complexity is <span>$O(n^2)$</span>, where <span>$n$</span> is the total number of samples.</p><p>Let <span>$(P_{X_i}, Y_i)_{i=1,\ldots,n}$</span> be a data set of predictions and corresponding targets. The estimator is defined as</p><p class="math-container">\[\frac{1}{n^2} \sum_{i,j=1}^n h_k\big((P_{X_i}, Y_i), (P_{X_j}, Y_j)\big)\]</p><p>where</p><p class="math-container">\[\begin{aligned}
h_k\big((Œº, y), (Œº&#39;, y&#39;)\big) ={}&amp;   k\big((Œº, y), (Œº&#39;, y&#39;)\big)
                                   - ùîº_{Z ‚àº Œº} k\big((Œº, Z), (Œº&#39;, y&#39;)\big) \\
                                 &amp; - ùîº_{Z&#39; ‚àº Œº&#39;} k\big((Œº, y), (Œº&#39;, Z&#39;)\big)
                                   + ùîº_{Z ‚àº Œº, Z&#39; ‚àº Œº&#39;} k\big((Œº, Z), (Œº&#39;, Z&#39;)\big).
\end{aligned}\]</p><p>The bias of the estimator is</p><p class="math-container">\[\frac{1}{n} \Big(\mathbb{E}_{P_X,Y} h_k\big((P_X, Y), (P_X, Y)\big) - \mathrm{SKCE}_k\Big).\]</p><p><strong>References</strong></p><p>Widmann, D., Lindsten, F., &amp; Zachariah, D. (2019). <a href="https://proceedings.neurips.cc/paper/2019/hash/1c336b8080f82bcc2cd2499b4c57261d-Abstract.html">Calibration tests in multi-class classification: A unifying framework</a>. In: Advances in Neural Information Processing Systems (NeurIPS 2019) (pp. 12257‚Äì12267).</p><p>Widmann, D., Lindsten, F., &amp; Zachariah, D. (2021). <a href="https://openreview.net/forum?id=-bxf89v3Nx">Calibration tests beyond classification</a>.</p><p>See also: <a href="#CalibrationErrors.UnbiasedSKCE"><code>UnbiasedSKCE</code></a>, <a href="#CalibrationErrors.BlockUnbiasedSKCE"><code>BlockUnbiasedSKCE</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/devmotion/CalibrationErrors.jl/blob/7df37174be61ac3d19ab1893f0ae78293d044cdd/src/skce/biased.jl#LL1-L46">source</a></section></article><h3 id="Unbiased-estimators"><a class="docs-heading-anchor" href="#Unbiased-estimators">Unbiased estimators</a><a id="Unbiased-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Unbiased-estimators" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="CalibrationErrors.UnbiasedSKCE" href="#CalibrationErrors.UnbiasedSKCE"><code>CalibrationErrors.UnbiasedSKCE</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">UnbiasedSKCE(k)</code></pre><p>Unbiased estimator of the squared kernel calibration error (SKCE) with kernel <code>k</code>.</p><p>Kernel <code>k</code> on the product space of predictions and targets has to be a <code>Kernel</code> from the Julia package <a href="https://github.com/JuliaGaussianProcesses/KernelFunctions.jl">KernelFunctions.jl</a> that can be evaluated for inputs that are tuples of predictions and targets.</p><p><strong>Details</strong></p><p>The estimator is unbiased and not guaranteed to be non-negative. Its sample complexity is <span>$O(n^2)$</span>, where <span>$n$</span> is the total number of samples.</p><p>Let <span>$(P_{X_i}, Y_i)_{i=1,\ldots,n}$</span> be a data set of predictions and corresponding targets. The estimator is defined as</p><p class="math-container">\[\frac{2}{n(n-1)} \sum_{1 \leq i &lt; j \leq n} h_k\big((P_{X_i}, Y_i), (P_{X_j}, Y_j)\big),\]</p><p>where</p><p class="math-container">\[\begin{aligned}
h_k\big((Œº, y), (Œº&#39;, y&#39;)\big) ={}&amp;   k\big((Œº, y), (Œº&#39;, y&#39;)\big)
                                   - ùîº_{Z ‚àº Œº} k\big((Œº, Z), (Œº&#39;, y&#39;)\big) \\
                                 &amp; - ùîº_{Z&#39; ‚àº Œº&#39;} k\big((Œº, y), (Œº&#39;, Z&#39;)\big)
                                   + ùîº_{Z ‚àº Œº, Z&#39; ‚àº Œº&#39;} k\big((Œº, Z), (Œº&#39;, Z&#39;)\big).
\end{aligned}\]</p><p><strong>References</strong></p><p>Widmann, D., Lindsten, F., &amp; Zachariah, D. (2019). <a href="https://proceedings.neurips.cc/paper/2019/hash/1c336b8080f82bcc2cd2499b4c57261d-Abstract.html">Calibration tests in multi-class classification: A unifying framework</a>. In: Advances in Neural Information Processing Systems (NeurIPS 2019) (pp. 12257‚Äì12267).</p><p>Widmann, D., Lindsten, F., &amp; Zachariah, D. (2021). <a href="https://openreview.net/forum?id=-bxf89v3Nx">Calibration tests beyond classification</a>.</p><p>See also: <a href="#CalibrationErrors.BiasedSKCE"><code>BiasedSKCE</code></a>, <a href="#CalibrationErrors.BlockUnbiasedSKCE"><code>BlockUnbiasedSKCE</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/devmotion/CalibrationErrors.jl/blob/7df37174be61ac3d19ab1893f0ae78293d044cdd/src/skce/unbiased.jl#LL1-L41">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CalibrationErrors.BlockUnbiasedSKCE" href="#CalibrationErrors.BlockUnbiasedSKCE"><code>CalibrationErrors.BlockUnbiasedSKCE</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">BlockUnbiasedSKCE(k[, blocksize = 2])</code></pre><p>Unbiased estimator of the squared kernel calibration error (SKCE) with kernel <code>k</code> that uses blocks with <code>blocksize</code> samples.</p><p>Kernel <code>k</code> on the product space of predictions and targets has to be a <code>Kernel</code> from the Julia package <a href="https://github.com/JuliaGaussianProcesses/KernelFunctions.jl">KernelFunctions.jl</a> that can be evaluated for inputs that are tuples of predictions and targets.</p><p>The number of samples per block must be at least 2 and at most the total number of samples. Note that samples in the last block are discarded if it is incomplete (see details below).</p><p><strong>Details</strong></p><p>The estimator is unbiased and not guaranteed to be non-negative. Its sample complexity is <span>$O(Bn)$</span>, where <span>$B$</span> is the block size and <span>$n$</span> is the total number of samples.</p><p>Let <span>$(P_{X_i}, Y_i)_{i=1,\ldots,n}$</span> be a data set of predictions and corresponding targets. The estimator with block size <span>$B$</span> is defined as</p><p class="math-container">\[{\bigg\lfloor \frac{n}{B} \bigg\rfloor}^{-1} \sum_{b=1}^{\lfloor n/B \rfloor}
\frac{2}{B(B-1)} \sum_{(b - 1) B &lt; i &lt; j \leq bB} h_k\big((P_{X_i}, Y_i), (P_{X_j}, Y_j)\big),\]</p><p>where</p><p class="math-container">\[\begin{aligned}
h_k\big((Œº, y), (Œº&#39;, y&#39;)\big) ={}&amp;   k\big((Œº, y), (Œº&#39;, y&#39;)\big)
                                   - ùîº_{Z ‚àº Œº} k\big((Œº, Z), (Œº&#39;, y&#39;)\big) \\
                                 &amp; - ùîº_{Z&#39; ‚àº Œº&#39;} k\big((Œº, y), (Œº&#39;, Z&#39;)\big)
                                   + ùîº_{Z ‚àº Œº, Z&#39; ‚àº Œº&#39;} k\big((Œº, Z), (Œº&#39;, Z&#39;)\big).
\end{aligned}\]</p><p><strong>References</strong></p><p>Widmann, D., Lindsten, F., &amp; Zachariah, D. (2019). <a href="https://proceedings.neurips.cc/paper/2019/hash/1c336b8080f82bcc2cd2499b4c57261d-Abstract.html">Calibration tests in multi-class classification: A unifying framework</a>. In: Advances in Neural Information Processing Systems (NeurIPS 2019) (pp. 12257‚Äì12267).</p><p>Widmann, D., Lindsten, F., &amp; Zachariah, D. (2021). <a href="https://openreview.net/forum?id=-bxf89v3Nx">Calibration tests beyond classification</a>.</p><p>See also: <a href="#CalibrationErrors.BiasedSKCE"><code>BiasedSKCE</code></a>, <a href="#CalibrationErrors.UnbiasedSKCE"><code>UnbiasedSKCE</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/devmotion/CalibrationErrors.jl/blob/7df37174be61ac3d19ab1893f0ae78293d044cdd/src/skce/unbiased.jl#LL53-L98">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-KSJ"><a class="tag is-link" href="#citeref-KSJ">KSJ</a>Kumar, A., Sarawagi, S., &amp; Jain, U. (2018). <a href="http://proceedings.mlr.press/v80/kumar18a.html">Trainable calibration measures for neural networks from kernel mean embeddings</a>. In <em>Proceedings of the 35th International Conference on Machine Learning</em> (pp. 2805-2814).</li><li class="footnote" id="footnote-WLZ19"><a class="tag is-link" href="#citeref-WLZ19">WLZ19</a>Widmann, D., Lindsten, F., &amp; Zachariah, D. (2019). <a href="https://proceedings.neurips.cc/paper/2019/hash/1c336b8080f82bcc2cd2499b4c57261d-Abstract.html">Calibration tests in multi-class classification: A unifying framework</a>. In <em>Advances in Neural Information Processing Systems 32 (NeurIPS 2019)</em> (pp. 12257‚Äì12267).</li><li class="footnote" id="footnote-WLZ21"><a class="tag is-link" href="#citeref-WLZ21">WLZ21</a>Widmann, D., Lindsten, F., &amp; Zachariah, D. (2021). <a href="https://openreview.net/forum?id=-bxf89v3Nx">Calibration tests beyond classification</a>. To be presented at <em>ICLR 2021</em>.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../ece/">¬´ Expected calibration error (ECE)</a><a class="docs-footer-nextpage" href="../others/">Other calibration errors ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 27 March 2021 12:09">Saturday 27 March 2021</span>. Using Julia version 1.6.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
