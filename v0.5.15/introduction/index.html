<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Introduction · CalibrationErrors.jl</title><link rel="canonical" href="https://devmotion.github.io/CalibrationErrors.jl/introduction/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">CalibrationErrors.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">CalibrationErrors.jl</a></li><li class="is-active"><a class="tocitem" href>Introduction</a><ul class="internal"><li><a class="tocitem" href="#Probabilistic-predictive-models"><span>Probabilistic predictive models</span></a></li><li><a class="tocitem" href="#Calibration"><span>Calibration</span></a></li><li><a class="tocitem" href="#Calibration-error"><span>Calibration error</span></a></li></ul></li><li><a class="tocitem" href="../ece/">Expected calibration error (ECE)</a></li><li><a class="tocitem" href="../kce/">Kernel calibration error (KCE)</a></li><li><a class="tocitem" href="../others/">Other calibration errors</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/classification/">Classification of penguin species</a></li><li><a class="tocitem" href="../examples/distribution/">Distribution of calibration error estimates</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Introduction</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Introduction</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/devmotion/CalibrationErrors.jl/blob/master/docs/src/introduction.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h1><h2 id="Probabilistic-predictive-models"><a class="docs-heading-anchor" href="#Probabilistic-predictive-models">Probabilistic predictive models</a><a id="Probabilistic-predictive-models-1"></a><a class="docs-heading-anchor-permalink" href="#Probabilistic-predictive-models" title="Permalink"></a></h2><p>A probabilistic predictive model predicts a probability distribution over a set of targets for a given feature. By predicting a distribution, one can express the uncertainty in the prediction, which might be inherent to the prediction task (e.g., if the feature does not contain enough information to determine the target with absolute certainty) or caused by insufficient knowledge of the underlying relation between feature and target (e.g., if only a small number of observations of features and corresponding targets are available).<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup></p><p>The following plot shows measurements of the bill length and the flipper length for three different penguin species in the <a href="https://github.com/allisonhorst/palmerpenguins">Palmer penguins dataset</a>.</p><p><img src="../examples/figures/penguins.svg" alt/></p><p>There exist many different probabilistic predictive models for predicting the probability of the penguin species (<em>target</em>) given the bill and flipper length (<em>feature</em>). We denote the feature by <span>$X$</span> and the target by <span>$Y$</span>, and let <span>$P_X$</span> be the prediction of a specific model <span>$P$</span> for a feature <span>$X$</span>. Ideally, we would like that</p><p class="math-container">\[P_X = \mathrm{law}(Y \,|\, X) \qquad \text{almost surely},\]</p><p>i.e., the model should predict the law of target <span>$Y$</span> given features <span>$X$</span>.<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup> Of course, usually it is not possible to achieve this in practice.</p><p>A very simple class of models are models that yield the same prediction for all features, i.e., they return the same probabilities for the penguin species regardless of the bill and flipper length. Clearly, more complicated take into account also the features and might output different predictions for different features.</p><p>In contrast to probabilistic predictive models, non-probabilistic predictive models predict a single target instead of a distribution over targets. In fact, such models can be viewed as a special class of probabilistic predictive models that output only Dirac distributions, i.e., that always predict 100% probability for one penguin species and 0% probability for all others.</p><p>Some other prediction models output a single target together with a confidence score between 0 and 1. Even these models can be reformulated as probabilistic predictive models, arguably in a slightly unconventional way: they correspond to a probabilistic model for a a binary classification problem whose feature space is extended with the predicted target and whose target is the predicted confidence score.</p><h2 id="Calibration"><a class="docs-heading-anchor" href="#Calibration">Calibration</a><a id="Calibration-1"></a><a class="docs-heading-anchor-permalink" href="#Calibration" title="Permalink"></a></h2><p>The main motivation for using a probabilistic model is that it provides additional information about the uncertainty of the predictions, which is valuable for decision making. A <a href="https://www.jstor.org/stable/2987588">classic example are weather forecasts</a> that also report the &quot;probability of rain&quot; instead of only if it will rain or not.</p><p>Therefore it is not sufficient if the model predicts an arbitrary distribution. Instead the predictions should actualy express the involved uncertainties &quot;correctly&quot;. One desired property is that the predictions are consistent: if the forecasts predict an 80% probability of rain for an infinite sequence of days, then ideally on 80% of the days it rains.</p><p>More generally, mathematically we would like</p><p class="math-container">\[P_X = \mathrm{law}(Y \,|\, P_X) \quad \text{almost surely},\]</p><p>i.e., the predicted distribution of targets should be equal to the distribution of targets conditioned on the predicted distribution.<sup class="footnote-reference"><a id="citeref-3" href="#footnote-3">[3]</a></sup></p><p>This statistical property is called <em>calibration</em>. If it is satisfied, a model is <em>calibrated</em>.</p><p>Obviously, the ideal model <span>$P_X = \mathrm{law}(Y \,|\, X)$</span> is calibrated. However, also the naive model <span>$P_X = \mathrm{law}(Y)$</span> that always predicts the marginal distribution of <span>$Y$</span> independent of the features is calibrated.<sup class="footnote-reference"><a id="citeref-4" href="#footnote-4">[4]</a></sup> In fact, any model of the form</p><p class="math-container">\[P_X = \mathrm{law}(Y \,|\, \phi(X)) \quad \text{almost surely},\]</p><p>where <span>$\phi$</span> is some measurable function, is calibrated.</p><h2 id="Calibration-error"><a class="docs-heading-anchor" href="#Calibration-error">Calibration error</a><a id="Calibration-error-1"></a><a class="docs-heading-anchor-permalink" href="#Calibration-error" title="Permalink"></a></h2><p>Calibration errors such as the <a href="../ece/#ece">expected calibration error</a> and the <a href="../kce/#kce">kernel calibration error</a> measure the calibration, or rather the degree of miscalibration, of probabilistic predictive models. They allow a more fine-tuned analysis of calibration and enable comparisons of calibration of different models. Intuitively, calibration measures quantify the deviation of <span>$P_X$</span> and <span>$\mathrm{law}(Y \,|\, P_X)$</span>, i.e., the left and right hand side in the calibration definition.</p><h3 id="Estimation"><a class="docs-heading-anchor" href="#Estimation">Estimation</a><a id="Estimation-1"></a><a class="docs-heading-anchor-permalink" href="#Estimation" title="Permalink"></a></h3><p>The calibration error of a model depends on the true conditional distribution of targets which is unknown in practice. Therefore calibration errors have to be estimated from a validation data set.</p><p>Various estimators of different calibration errors such as the <a href="../ece/#ece">expected calibration error</a> and the <a href="../kce/#kce">kernel calibration error</a> are implemented in CalibrationErrors.</p><article class="docstring"><header><a class="docstring-binding" id="CalibrationErrors.CalibrationErrorEstimator" href="#CalibrationErrors.CalibrationErrorEstimator"><code>CalibrationErrors.CalibrationErrorEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">(estimator::CalibrationErrorEstimator)(predictions, targets)</code></pre><p>Estimate the calibration error of a model from the set of <code>predictions</code> and corresponding <code>targets</code> using the <code>estimator</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/devmotion/CalibrationErrors.jl/blob/7179cba20ab3cda785c755c5f19c93cd8d4a4b94/src/generic.jl#LL3-L8">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>It does not matter how the model is obtained. In particular, both Bayesian and frequentist approaches can be used.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>In classification problems, the law <span>$\mathrm{law}(Y \,|\, X)$</span> can be identified with a vector in the probability simplex. Therefore often we just consider this equivalent formulation, both for the predictions <span>$P_X$</span> and the law <span>$\mathrm{law}(Y \,|\, X)$</span>.</li><li class="footnote" id="footnote-3"><a class="tag is-link" href="#citeref-3">3</a>The general formulation applies not only to classification but to any prediction task with arbitrary target spaces, including regression.</li><li class="footnote" id="footnote-4"><a class="tag is-link" href="#citeref-4">4</a>In meterology, this model is called the <em>climatology</em>.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« CalibrationErrors.jl</a><a class="docs-footer-nextpage" href="../ece/">Expected calibration error (ECE) »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 27 April 2021 12:48">Tuesday 27 April 2021</span>. Using Julia version 1.6.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
