<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Classification of penguin species · CalibrationErrors.jl</title><link rel="canonical" href="https://devmotion.github.io/CalibrationErrors.jl/examples/classification/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">CalibrationErrors.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">CalibrationErrors.jl</a></li><li><a class="tocitem" href="../../introduction/">Introduction</a></li><li><a class="tocitem" href="../../ece/">Expected calibration error (ECE)</a></li><li><a class="tocitem" href="../../kce/">Kernel calibration error (KCE)</a></li><li><a class="tocitem" href="../../others/">Other calibration errors</a></li><li><span class="tocitem">Examples</span><ul><li class="is-active"><a class="tocitem" href>Classification of penguin species</a><ul class="internal"><li><a class="tocitem" href="#Packages"><span>Packages</span></a></li><li><a class="tocitem" href="#Data"><span>Data</span></a></li><li><a class="tocitem" href="#Fitting-normal-distributions"><span>Fitting normal distributions</span></a></li><li><a class="tocitem" href="#Naive-Bayes-classifier"><span>Naive Bayes classifier</span></a></li><li><a class="tocitem" href="#Evaluation"><span>Evaluation</span></a></li></ul></li><li><a class="tocitem" href="../distribution/">Distribution of calibration error estimates</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Classification of penguin species</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Classification of penguin species</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/devmotion/CalibrationErrors.jl/blob/master/examples/classification/script.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Classification-of-penguin-species"><a class="docs-heading-anchor" href="#Classification-of-penguin-species">Classification of penguin species</a><a id="Classification-of-penguin-species-1"></a><a class="docs-heading-anchor-permalink" href="#Classification-of-penguin-species" title="Permalink"></a></h1><p><a href="https://nbviewer.jupyter.org/github/devmotion/CalibrationErrors.jl/blob/gh-pages/v0.5.19/examples/classification.ipynb"><img src="https://img.shields.io/badge/show-nbviewer-579ACA.svg" alt/></a></p><p>You are seeing the HTML output generated by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a> from the <a href="https://github.com/devmotion/CalibrationErrors.jl/blob/master/examples/classification/script.jl">Julia source file</a>. The corresponding notebook can be viewed in <a href="https://nbviewer.jupyter.org/github/devmotion/CalibrationErrors.jl/blob/gh-pages/v0.5.19/examples/classification.ipynb">nbviewer</a>.</p><h2 id="Packages"><a class="docs-heading-anchor" href="#Packages">Packages</a><a id="Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Packages" title="Permalink"></a></h2><pre><code class="language-julia">using CairoMakie
using CalibrationErrors
using DataFrames
using Distances
using Distributions
using PalmerPenguins
using Query

using Random

using CairoMakie.AbstractPlotting.ColorSchemes: Dark2_8</code></pre><h2 id="Data"><a class="docs-heading-anchor" href="#Data">Data</a><a id="Data-1"></a><a class="docs-heading-anchor-permalink" href="#Data" title="Permalink"></a></h2><p>In this example we study the calibration of different models that classify three penguin species based on measurements of their bill and flipper lengths.</p><p>We use the <a href="https://allisonhorst.github.io/palmerpenguins/">Palmer penguins dataset</a> to to train and validate the models.</p><pre><code class="language-julia">penguins = dropmissing(DataFrame(PalmerPenguins.load()))

f = Figure()
ax = Axis(f[1, 1]; xlabel=&quot;bill length [mm]&quot;, ylabel=&quot;flipper length [mm]&quot;)
for (i, (key, df)) in enumerate(pairs(groupby(penguins, :species)))
    scatter!(
        df.bill_length_mm, df.flipper_length_mm; color=(Dark2_8[i], 0.8), label=key.species
    )
end
Legend(f[1, 2], ax, &quot;species&quot;)
save(&quot;./figures/penguins.svg&quot;, f);</code></pre><p><img src="../figures/penguins.svg" alt/></p><p>We split the data randomly into a training and validation dataset. The training dataset contains around 60% of the samples.</p><pre><code class="language-julia">Random.seed!(1234)
idxs = shuffle(axes(penguins, 1))
k = floor(Int, 0.6 * length(idxs))
train_idxs = @view idxs[1:k]
val_idxs = @view idxs[(k + 1):end]

train_penguins = penguins[train_idxs, :]
val_penguins = penguins[val_idxs, :];</code></pre><pre><code class="language-julia">f = Figure()
ax = Axis(f[1, 1]; xlabel=&quot;bill length [mm]&quot;, ylabel=&quot;flipper length [mm]&quot;)
for (i, df) in enumerate((train_penguins, val_penguins))
    for (j, (key, subdf)) in enumerate(pairs(groupby(df, :species)))
        scatter!(
            subdf.bill_length_mm,
            subdf.flipper_length_mm;
            color=(Dark2_8[j], 0.8),
            marker=i == 1 ? :circle : :diamond,
        )
    end
end
group_marker = [
    MarkerElement(; marker=m, color=:black, strokecolor=:transparent, markersize=20) for
    m in (:circle, :diamond)
]
group_color = [PolyElement(; color=Dark2_8[i], strokecolor=:transparent) for i in 1:3]
Legend(
    f[1, 2],
    [group_marker, group_color],
    [[&quot;training&quot;, &quot;validation&quot;], string.(levels(penguins.species))],
    [&quot;dataset&quot;, &quot;species&quot;],
)
save(&quot;./figures/penguins_datasets.svg&quot;, f);</code></pre><p><img src="../figures/penguins_datasets.svg" alt/></p><h2 id="Fitting-normal-distributions"><a class="docs-heading-anchor" href="#Fitting-normal-distributions">Fitting normal distributions</a><a id="Fitting-normal-distributions-1"></a><a class="docs-heading-anchor-permalink" href="#Fitting-normal-distributions" title="Permalink"></a></h2><p>For each species, we fit independent normal distributions to the observations of the bill and flipper length in the training data, using maximum likelihood estimation.</p><pre><code class="language-julia">penguins_fit = @from i in train_penguins begin
    @group i by i.species into g
    @select {
        species = key(g),
        proportion = length(g) / nrow(train_penguins),
        bill = fit(Normal, g.bill_length_mm),
        flipper = fit(Normal, g.flipper_length_mm),
    }
    @collect DataFrame
end</code></pre><table class="data-frame"><thead><tr><th></th><th>species</th><th>proportion</th><th>bill</th><th>flipper</th></tr><tr><th></th><th>String</th><th>Float64</th><th>Normal…</th><th>Normal…</th></tr></thead><tbody><p>3 rows × 4 columns</p><tr><th>1</th><td>Adelie</td><td>0.462312</td><td>Distributions.Normal{Float64}(μ=38.6891, σ=2.76643)</td><td>Distributions.Normal{Float64}(μ=190.152, σ=6.50908)</td></tr><tr><th>2</th><td>Gentoo</td><td>0.356784</td><td>Distributions.Normal{Float64}(μ=47.3479, σ=3.05664)</td><td>Distributions.Normal{Float64}(μ=216.746, σ=6.28371)</td></tr><tr><th>3</th><td>Chinstrap</td><td>0.180905</td><td>Distributions.Normal{Float64}(μ=48.5194, σ=3.42481)</td><td>Distributions.Normal{Float64}(μ=195.139, σ=7.90974)</td></tr></tbody></table><p>We plot the estimated normal distributions.</p><pre><code class="language-julia">function xrange(dists, alpha=0.0001)
    xmin = minimum(Base.Fix2(quantile, alpha), dists)
    xmax = maximum(Base.Fix2(quantile, 1 - alpha), dists)
    return range(xmin, xmax; length=1_000)
end

function plot_normal_fit(dists, species, xlabel)
    f = Figure()
    Axis(f[1, 1]; xlabel=xlabel, ylabel=&quot;density&quot;)
    xs = xrange(dists)
    plots = map(enumerate(dists)) do (i, dist)
        ys = pdf.(dist, xs)
        l = lines!(xs, ys; color=Dark2_8[i])
        b = band!(xs, 0, ys; color=(Dark2_8[i], 0.2))
        return [l, b]
    end
    Legend(f[1, 2], plots, species, &quot;species&quot;)
    return f
end

plot_normal_fit(penguins_fit.bill, penguins_fit.species, &quot;bill length [mm]&quot;)
save(&quot;./figures/normal_fit_bill.svg&quot;, current_figure());</code></pre><p><img src="../figures/normal_fit_bill.svg" alt/></p><pre><code class="language-julia">plot_normal_fit(penguins_fit.flipper, penguins_fit.species, &quot;flipper length [mm]&quot;)
save(&quot;./figures/normal_fit_flipper.svg&quot;, current_figure());</code></pre><p><img src="../figures/normal_fit_flipper.svg" alt/></p><h2 id="Naive-Bayes-classifier"><a class="docs-heading-anchor" href="#Naive-Bayes-classifier">Naive Bayes classifier</a><a id="Naive-Bayes-classifier-1"></a><a class="docs-heading-anchor-permalink" href="#Naive-Bayes-classifier" title="Permalink"></a></h2><p>Let us assume that the bill and flipper length are conditionally independent given the penguin species. Then Bayes&#39; theorem implies that</p><p class="math-container">\[\begin{aligned}
\mathbb{P}(\mathrm{species} \,|\, \mathrm{bill}, \mathrm{flipper})
&amp;= \frac{\mathbb{P}(\mathrm{species}) \mathbb{P}(\mathrm{bill}, \mathrm{flipper} \,|\, \mathrm{species})}{\mathbb{P}(\mathrm{bill}, \mathrm{flipper})} \\
&amp;= \frac{\mathbb{P}(\mathrm{species}) \mathbb{P}(\mathrm{bill} \,|\, \mathrm{species}) \mathbb{P}(\mathrm{flipper} \,|\, \mathrm{species})}{\mathbb{P}(\mathrm{bill}, \mathrm{flipper})}.
\end{aligned}\]</p><p>This predictive model is known as <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">naive Bayes classifier</a>.</p><p>In the section above, we estimated <span>$\mathbb{P}(\mathrm{species})$</span>, <span>$\mathbb{P}(\mathrm{bill} \,|\, \mathrm{species})$</span>, and <span>$\mathbb{P}(\mathrm{flipper} \,|\, \mathrm{species})$</span> for each penguin species from the training data. For the conditional distributions we used a Gaussian approximation.</p><pre><code class="language-julia">function predict_naive_bayes_classifier(fit, data)
    # Compute unnormalized probabilities
    z =
        log.(permutedims(fit.proportion)) .+
        logpdf.(permutedims(fit.bill), data.bill_length_mm) .+
        logpdf.(permutedims(fit.flipper), data.flipper_length_mm)

    # Normalize probabilities
    u = maximum(z; dims=2)
    z .= exp.(z .- u)
    sum!(u, z)
    z ./= u

    return DataFrame(z, fit.species)
end

train_predict = predict_naive_bayes_classifier(penguins_fit, train_penguins)
val_predict = predict_naive_bayes_classifier(penguins_fit, val_penguins);</code></pre><h2 id="Evaluation"><a class="docs-heading-anchor" href="#Evaluation">Evaluation</a><a id="Evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluation" title="Permalink"></a></h2><p>We evaluate the probabilistic predictions of the naive Bayes classifier that we just trained. It is easier to work with a numerical encoding of the true penguin species and a corresponding vector of predictions.</p><pre><code class="language-julia">train_species = convert(Vector{Int}, indexin(train_penguins.species, names(train_predict)))
train_probs = RowVecs(Matrix{Float64}(train_predict))

val_species = convert(Vector{Int}, indexin(val_penguins.species, names(val_predict)))
val_probs = RowVecs(Matrix{Float64}(val_predict));</code></pre><h3 id="Log-likelihood"><a class="docs-heading-anchor" href="#Log-likelihood">Log-likelihood</a><a id="Log-likelihood-1"></a><a class="docs-heading-anchor-permalink" href="#Log-likelihood" title="Permalink"></a></h3><p>We compute the average log-likelihood of the training and validation data. It is equivalent to the negative cross-entropy.</p><pre><code class="language-julia">function mean_loglikelihood(species, probs)
    return mean(log(p[s]) for (s, p) in zip(species, probs))
end

mean_loglikelihood(train_species, train_probs)</code></pre><pre class="documenter-example-output">-0.17256012868504889</pre><pre><code class="language-julia">mean_loglikelihood(val_species, val_probs)</code></pre><pre class="documenter-example-output">-0.14178608137216686</pre><h3 id="Brier-score"><a class="docs-heading-anchor" href="#Brier-score">Brier score</a><a id="Brier-score-1"></a><a class="docs-heading-anchor-permalink" href="#Brier-score" title="Permalink"></a></h3><p>The average log-likelihood is also equivalent to the <a href="https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf">logarithmic score</a>. The Brier score is another strictly proper scoring rule that can be used for evaluating probabilistic predictions.</p><pre><code class="language-julia">function brier_score(species, probs)
    return mean(
        sum(abs2(pi - (i == s)) for (i, pi) in enumerate(p)) for
        (s, p) in zip(species, probs)
    )
end

brier_score(train_species, train_probs)</code></pre><pre class="documenter-example-output">0.09151465062557913</pre><pre><code class="language-julia">brier_score(val_species, val_probs)</code></pre><pre class="documenter-example-output">0.07692851868300807</pre><h3 id="Expected-calibration-error"><a class="docs-heading-anchor" href="#Expected-calibration-error">Expected calibration error</a><a id="Expected-calibration-error-1"></a><a class="docs-heading-anchor-permalink" href="#Expected-calibration-error" title="Permalink"></a></h3><p>As all proper scoring rules, the logarithmic and the Brier score can be <a href="https://doi.org/10.1002/qj.456">decomposed in three terms that quantify the sharpness and calibration of the predictive model and the irreducible uncertainty of the targets that is inherent to the prediction problem</a>. The calibration term in this decomposition is the expected calibration error (ECE)</p><p class="math-container">\[\mathbb{E} d\big(P_X, \mathrm{law}(Y \,|\, P_X)\big)\]</p><p>with respect to the score divergence <span>$d$</span>.</p><p>Scoring rules, however, include also the sharpness and the uncertainty term. Thus models can trade off calibration for sharpness and therefore scoring rules are not suitable for specifically evaluating calibration of predictive models.</p><p>The score divergence to the logarithmic and the Brier score are the Kullback-Leibler (KL) divergence</p><p class="math-container">\[d\big(P_X, \mathrm{law}(Y \,|\, P_X)\big) = \sum_{y} \mathbb{P}(Y = y \,|\, P_X)
\log\big(\mathbb{P}(Y = y \,|\, P_X) / P_X(\{y\})\big)\]</p><p>and the squared Euclidean distance</p><p class="math-container">\[d\big(P_X, \mathrm{law}(Y \,|\, P_X)\big) = \sum_{y} \big(P_X - \mathrm{law}(Y \,|\, P_X)\big)^2(\{y\}),\]</p><p>respectively. The KL divergence is defined only if <span>$\mathrm{law}(Y \,|\, P_X)$</span> is absolutely continuous with respect to <span>$P_X$</span>, i.e., if <span>$P_X(\{y\}) = 0$</span> implies <span>$\mathbb{P}(Y = y \,|\, P_X) = 0$</span>.</p><p>We estimate the ECE by binning the probability simplex of predictions <span>$P_X$</span> and computing the weighted average of the distances between the mean prediction and the distribution of targets in each bin.</p><p>One approach is to use bins of uniform size.</p><pre><code class="language-julia">ece = ECE(UniformBinning(10), (μ, y) -&gt; kl_divergence(y, μ))
ece(train_probs, train_species)</code></pre><pre class="documenter-example-output">0.06734377731547585</pre><pre><code class="language-julia">ece(val_probs, val_species)</code></pre><pre class="documenter-example-output">0.0357151309862163</pre><p>For the squared Euclidean distance we obtain:</p><pre><code class="language-julia">ece = ECE(UniformBinning(10), SqEuclidean())
ece(train_probs, train_species)</code></pre><pre class="documenter-example-output">0.023481249069090715</pre><pre><code class="language-julia">ece(val_probs, val_species)</code></pre><pre class="documenter-example-output">0.014088367573456663</pre><p>Alternatively, one can use a data-dependent binning scheme that tries to split the predictions in a way that minimizes the variance in each bin.</p><p>With the KL divergence we get:</p><pre><code class="language-julia">ece = ECE(MedianVarianceBinning(5), (μ, y) -&gt; kl_divergence(y, μ))
ece(train_probs, train_species)</code></pre><pre class="documenter-example-output">0.07499919743918855</pre><pre><code class="language-julia">ece(val_probs, val_species)</code></pre><pre class="documenter-example-output">0.02139440771337293</pre><p>For the squared Euclidean distance we obtain:</p><pre><code class="language-julia">ece = ECE(MedianVarianceBinning(5), SqEuclidean())
ece(train_probs, train_species)</code></pre><pre class="documenter-example-output">0.020821939141060335</pre><pre><code class="language-julia">ece(val_probs, val_species)</code></pre><pre class="documenter-example-output">0.0043922685321516565</pre><p>We see that the estimates (of the same theoretical quantity!) are highly dependent on the chosen binning scheme.</p><h3 id="Kernel-calibration-error"><a class="docs-heading-anchor" href="#Kernel-calibration-error">Kernel calibration error</a><a id="Kernel-calibration-error-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-calibration-error" title="Permalink"></a></h3><p>As an alternative to the ECE, we estimate the kernel calibration error (KCE). We keep it simple here, and use the tensor product kernel</p><p class="math-container">\[k\big((\mu, y), (\mu&#39;, y&#39;)\big) = \delta_{y,y&#39;} \exp{\bigg(-\frac{{\|\mu - \mu&#39;\|}_2^2}{2\nu^2} \bigg)}\]</p><p>with length scale <span>$\nu &gt; 0$</span> for predictions <span>$\mu,\mu&#39;$</span> and corresponding targets <span>$y, y&#39;$</span>. For simplicity, we estimate length scale <span>$\nu$</span> with the median heuristic.</p><pre><code class="language-julia">distances = pairwise(SqEuclidean(), train_probs)
λ = sqrt(median(distances[i] for i in CartesianIndices(distances) if i[1] &lt; i[2]))
kernel = (GaussianKernel() ∘ ScaleTransform(inv(λ))) ⊗ WhiteKernel();</code></pre><p>We obtain the following biased estimates of the squared KCE (SKCE):</p><pre><code class="language-julia">skce = BiasedSKCE(kernel)
skce(train_probs, train_species)</code></pre><pre class="documenter-example-output">0.00019771243779825737</pre><pre><code class="language-julia">skce(val_probs, val_species)</code></pre><pre class="documenter-example-output">0.0001799014357104042</pre><p>Similar to the biased estimates of the ECE, the biased estimates of the SKCE are always non-negative. The unbiased estimates can be negative as well, in particular if the model is (close to being) calibrated:</p><pre><code class="language-julia">skce = UnbiasedSKCE(kernel)
skce(train_probs, train_species)</code></pre><pre class="documenter-example-output">-0.0002634842197157877</pre><pre><code class="language-julia">skce(val_probs, val_species)</code></pre><pre class="documenter-example-output">-0.00039715583682566714</pre><p>When the datasets are large, the quadratic sample complexity of the standard biased and unbiased estimators of the SKCE can become prohibitive. In these cases, one can resort to an estimator that averages estimates of non-overlapping blocks of samples. This estimator allows to trade off computational cost for increased variance.</p><p>Here we consider the extreme case of blocks with two samples, which yields an estimator with linear sample complexity:</p><pre><code class="language-julia">skce = BlockUnbiasedSKCE(kernel, 2)
skce(train_probs, train_species)</code></pre><pre class="documenter-example-output">-0.0019950778379260034</pre><pre><code class="language-julia">skce(val_probs, val_species)</code></pre><pre class="documenter-example-output">-0.0008908675271914782</pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../others/">« Other calibration errors</a><a class="docs-footer-nextpage" href="../distribution/">Distribution of calibration error estimates »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Sunday 16 May 2021 21:23">Sunday 16 May 2021</span>. Using Julia version 1.6.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
